# LLM-driven Web Crawler & Knowledge Miner

A modular, LLM-augmented pipeline to discover, scrape, extract, and expand knowledge on any topic using recursive search-engine queries, web scraping, and LLM-powered topic/entity extraction.

---

## üöÄ Table of Contents

* [Overview](#overview)
* [Features](#features)
* [Architecture & Design](#architecture--design)
* [Directory Structure](#directory-structure)
* [Installation](#installation)
* [Configuration](#configuration)
* [Usage](#usage)
* [Pipeline Details](#pipeline-details)
* [Extending the Knowledge Base](#extending-the-knowledge-base)
* [Exploration vs. Exploitation](#exploration-vs-exploitation)
* [Roadmap & To Do](#roadmap--to-do)
* [Contributing](#contributing)
* [License](#license)

---

## üìñ Overview

This project provides a fully automated research engine that:

1. **Crawls** the web via a search‚Äëengine API (e.g., SearXNG).
2. **Scrapes** HTML pages (and PDFs) using Playwright and BeautifulSoup.
3. **Extracts** structured content and media metadata in LLM‚Äëfriendly formats.
4. **Builds** a queryable knowledge base of topics, entities, contexts, and concepts.
5. **Expands** with new search queries via LLM‚Äëdriven query rewriting and exploration/exploitation strategies.

By combining search, scraping, and LLMs, you can bootstrap a focused knowledge graph or document store on any subject.

---

## ‚ú® Features

* **Crawler** (`core/crawler.py`)

  * Batch queries to SearXNG or other engines.
  * Deduplicates using a persistent `shelve` index.
  * Checkpointing to JSONL for safe writes.

* **Scraper** (`core/scraper.py`)

  * Async fetching via Playwright (headless Chromium).
  * Smart scrolling + ‚Äúload more‚Äù clicks.
  * Text extraction with headings, paragraphs, lists, and markdown output.
  * Image metadata extraction & validation.

* **Knowledge Extraction** (`core/topic_extraction.py`)

  * (PLANNED) LLM-driven extraction of topics, entities, and concept graphs.

* **Query Transformation** (`main.py`)

  * Baseline query templates (e.g., adding ‚Äútravel tips‚Äù, ‚Äúlandmarks‚Äù).
  * (PLANNED) LLM-based query rewriting for exploration/exploitation.

* **Storage** (`storage/`)

  * JSONL/text files for scraped links and content.
  * `shelve` for fast URL deduplication.

* **Utilities** (`utils/`)

  * Custom logger, link loader, and other helpers.

---

## üèóÔ∏è Architecture & Design

```
[ User Query ]
      |
      v                +---------------------------+
[ Query Rewriter ]--> |  Core Crawler (SearXNG)   | --> JSONL links + shelve index
      |
      v                +---------------------------+
[ Expanded Queries ]
      |
      v                +---------------------------+
[ Core Scraper ]----> |  Playwright + BS4         | --> Markdown + images JSON
      |                +---------------------------+
      v
[ Topic Extraction ] (LLM)
      v
[ Knowledge Base ] (vector store / graph / JSON)
      |
      v
[ Next-Gen Query Expansion ]
```

1. **Initial Seed** ‚Äî  start from a root topic.
2. **Query Expansion** ‚Äî LLM rewrites to generate sub‚Äëqueries.
3. **Crawl & Scrape** ‚Äî fetch links & content, store metadata.
4. **Topic Extraction** ‚Äî invoke an LLM to pull out entities, themes.
5. **Knowledge Base** ‚Äî index results (e.g., vector DB) for search.
6. **Recursive Loop** ‚Äî pick new queries (explore/exploit) and repeat.

---

## üìÇ Directory Structure

```
app/
‚îú‚îÄ‚îÄ configs/             # JSON schema & default config
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ crawler.py       # Search-engine crawler
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py       # Async web scraper & extractor
‚îÇ   ‚îî‚îÄ‚îÄ topic_extraction.py  # LLM-based topic/entity extractor (TBD)
‚îú‚îÄ‚îÄ storage/
‚îÇ   ‚îú‚îÄ‚îÄ links.jsonl      # Collected link metadata
‚îÇ   ‚îú‚îÄ‚îÄ markdown.json    # Scraped page content
‚îÇ   ‚îî‚îÄ‚îÄ images.json      # Extracted image metadata
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ logger.py        # Unified logging
‚îÇ   ‚îî‚îÄ‚îÄ utils.py         # Helpers (link loading, etc.)
‚îú‚îÄ‚îÄ main.py              # CLI entrypoint & pipeline orchestration
‚îî‚îÄ‚îÄ README.md            # This file
```

---

## ‚öôÔ∏è Installation

1. **Clone** the repo:

   ```bash
   git clone https://github.com/your-org/your-repo.git
   cd your-repo
   ```

2. **Create & activate** a virtual environment:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate
   ```

3. **Install** dependencies:

   ```bash
   pip install -r requirements.txt
   ```

4. **Update your `.gitignore`** to exclude your virtual environment folder (e.g., `.venv/`) and other temporary files.

5. **Run** SearXNG locally (or point at another engine) on port `8124`.

## üîß Configuration

This pipeline relies on a **self‚Äëhosted SearxNG** instance as its meta‚Äësearch engine. Follow the official installation guide:

* **SearxNG Admin Installation (Docker)**: [https://docs.searxng.org/admin/installation-docker.html](https://docs.searxng.org/admin/installation-docker.html)
* **Docker Image**: [https://hub.docker.com/r/searxng/searxng](https://hub.docker.com/r/searxng/searxng)

Once deployed (default port `8124`), update your `config.json` so that the `crawler.searxng_url` matches your SearxNG endpoint. Example config:

```json
{
  "crawler": {
    "searxng_url": "http://localhost:8124/search",
    "query": "Singapore",
    "language": "en",
    "pages": 1,
    "time_range": "year",
    "timeout": 3,
    "links_file_path": "app/storage/raw_links/links.jsonl",
    "shelf_path": "app/storage/raw_links/db_link_hashing/link_hash_db"
  },
  "scraper": {
    "concurrency": 4,
    "links_file_path": "app/storage/raw_links/links.jsonl",
    "images_outfile": "app/storage/images_metadata/images_metadata.json",
    "markdown_outfile": "app/storage/images_metadata/text_markdown.json"
  }
}
```

> **‚ö†Ô∏è Before running**: ensure your SearxNG container is up and the `searxng_url` is reachable (e.g., `curl http://localhost:8124/search?q=test`).

## üöÄ Usage

Run the full pipeline from your terminal:

```bash
python -m app.main --config app/configs/config.json
```

Or, import & call individual modules:

```python
from app.core.crawler import Crawler
from app.utils.utils import load_links

# 1. Crawl links
tool = Crawler(config.crawler)
new = tool.search_and_store_batch(["singapore attractions", "singapore food"])

# 2. Scrape content
links = load_links(config.scraper.links_file_path)
async with Scraper(config.scraper) as s:
    results = await s.extract_all_content(links)
```

---

## üß† Extending the Knowledge Base

* **Topic Extraction**: Use `core/topic_extraction.py` to feed scraped markdown into an LLM (OpenAI, Claude, etc.)

  * Extract **entities**, **topics**, **concepts**, **relations**.
  * Store in your vector DB or graph database.

* **Query Rewriting**: Replace the placeholder `transform_query()` in `main.py` with an LLM prompt that:

  1. **Exploits** high‚Äëconfidence areas (common topics).
  2. **Explores** sparse or niche subtopics.
  3. Ranks & returns top‚ÄëK new queries.

* **Exploitation vs. Exploration**:

  * Exploitation: repeat queries around frequent entities to deepen coverage.
  * Exploration: generate novel queries for long‚Äëtail or emerging terms.
  * Use heuristic scoring (e.g., TF-IDF on your KB) or ask the LLM to rate.

---

## üìà Exploration vs. Exploitation Strategies

1. **Frequency-based**: prioritize queries containing high‚Äëfrequency entities (exploit) vs. low‚Äëfrequency (explore).
2. **Recency-based**: use time\_range filters to surface fresh content.
3. **Diversity-aware**: ask the LLM to generate semantically diverse queries.

> Tip: prompt the LLM with your current KB summary, then ask:
>
> ```text
> "Given this list of discovered topics: [A, B, C, ...], suggest five new search queries that explore lesser-covered areas."
> ```

---

## üõ£Ô∏è Roadmap & To Do

* [x] Basic crawler implementation (`core/crawler.py`)
* [x] Basic scraper implementation (`core/scraper.py`)
* [x] Baseline query templates in `main.py`
* [ ] LLM-driven query rewriting (in `process.py`)
* [ ] Exploration/exploitation scoring and logic for query expansion
* [ ] Topic/entity extraction module (`core/topic_extraction.py`)
* [ ] PDF content support in `scraper.py`
* [ ] Checkpointing & resume support for long-running crawls
* [ ] Integration with a vector database or graph store for the knowledge base
* [ ] Recursive query-driven pipeline loop
* [ ] Detailed documentation and usage examples for each component

<!-- ## ü§ù Contributing

1. Fork & clone.
2. Create a feature branch (`git checkout -b feature/xyz`).
3. Commit your changes and push.
4. Open a Pull Request describing your enhancement.

Please follow our [Code of Conduct](./CODE_OF_CONDUCT.md).

---

## üìú License

This project is licensed under the [MIT License](./LICENSE). -->
