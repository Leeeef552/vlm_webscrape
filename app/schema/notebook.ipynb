{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d793919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "from hashlib import md5\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert entity classification system. Your task is to classify entities into predefined categories.\n",
    "\n",
    "## INSTRUCTIONS:\n",
    "1. You will receive a list of predefined labels at the beginning\n",
    "2. For each entity provided, you must select exactly ONE label from the predefined list\n",
    "3. Classifications must be from the exact predefined labels list\n",
    "4. If an entity doesn't clearly fit any label, choose the most appropriate one\n",
    "\n",
    "## PREDEFINED LABELS:\n",
    "{LABELS}\n",
    "\n",
    "## RESPONSE FORMAT:\n",
    "For each entity, provide exactly ONE label from the predefined list followed by a confidence score in this exact format:\n",
    "\"LABEL_NAME|CONFIDENCE_SCORE\"\n",
    "\n",
    "Where CONFIDENCE_SCORE is a number between 0 and 1 (e.g., \"Cemetery|0.95\")\n",
    "\n",
    "## CRITERIA FOR SELECTION:\n",
    "- Choose only from the predefined labels above\n",
    "- Select the most relevant label based on entity description\n",
    "- If multiple labels seem relevant, choose the most specific/precise one\n",
    "- If unsure, make your best educated guess from the available options\n",
    "- The label must be an exact match to one of the predefined labels\n",
    "- Confidence score should reflect how certain you are in your choice (0.0 = no knowledge at all, 0.5 = uncertain, 1.0 = very certain)\n",
    "\n",
    "## IMPORTANT:\n",
    "- ONLY return ONE label that exists in the predefined list\n",
    "- Do not create new labels or variations\n",
    "- Do not return anything other than exactly one valid label followed by a pipe and confidence score\n",
    "- The label must be a complete string matching exactly one of the predefined labels\n",
    "- The confidence score must be a decimal between 0 and 1\n",
    "- Return format: \"LABEL_NAME|CONFIDENCE_SCORE\" - nothing else\n",
    "\n",
    "## ENTITY TO CLASSIFY:\n",
    "{ENTITY_DATA}\n",
    "\"\"\"\n",
    "\n",
    "def load_labels_from_jsonl(file_path):\n",
    "    \"\"\"Load labels from JSONL file\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            labels.append(data.get('label', data.get('name', '')))\n",
    "    return labels\n",
    "\n",
    "def create_system_prompt(labels_list, entity_data):\n",
    "    \"\"\"Create system prompt with labels\"\"\"\n",
    "    labels_str = \"\\n\".join([f\"- {label}\" for label in labels_list])\n",
    "    return SYSTEM_PROMPT.format(\n",
    "        LABELS=labels_str,\n",
    "        ENTITY_DATA=entity_data\n",
    "    )\n",
    "\n",
    "def get_entity_hash(entity_data):\n",
    "    \"\"\"Create a hash of the entity data for deduplication\"\"\"\n",
    "    entity_string = entity_data.get('entity', '').lower().strip()\n",
    "    return md5(entity_string.encode()).hexdigest()\n",
    "\n",
    "def deduplicate_entities(entities):\n",
    "    \"\"\"Remove duplicate entities based on the 'entity' field\"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique_entities = []\n",
    "    duplicates = []\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_hash = get_entity_hash(entity)\n",
    "        if entity_hash not in seen_hashes:\n",
    "            seen_hashes.add(entity_hash)\n",
    "            unique_entities.append(entity)\n",
    "        else:\n",
    "            duplicates.append(entity)\n",
    "\n",
    "    print(f\"Removed {len(duplicates)} duplicate entities\")\n",
    "    print(f\"Kept {len(unique_entities)} unique entities\")\n",
    "\n",
    "    return unique_entities\n",
    "\n",
    "def process_entity_with_pass5(client, entity_data, labels_list):\n",
    "    \"\"\"Generate 5 classifications for an entity\"\"\"\n",
    "    simplified_data = {\n",
    "        \"entity\": entity_data.get(\"entity\", \"\"),\n",
    "        \"description\": entity_data.get(\"description\", \"\")\n",
    "    }\n",
    "\n",
    "    system_prompt = create_system_prompt(labels_list, json.dumps(simplified_data))\n",
    "\n",
    "    responses = []\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"google/gemma-3-12b-it\",  # Replace with your actual model name\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this entity: {json.dumps(simplified_data)}\"}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "            responses.append(response_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response {i+1}: {e}\")\n",
    "            responses.append(None)\n",
    "\n",
    "    return responses\n",
    "\n",
    "def parse_response_with_confidence(response_text):\n",
    "    \"\"\"Parse response to extract label and confidence score\"\"\"\n",
    "    if not response_text or \"|\" not in response_text:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        parts = response_text.split(\"|\", 1)\n",
    "        if len(parts) >= 2:\n",
    "            label = parts[0].strip().strip('\"\\'')\n",
    "            confidence_str = parts[1].strip().strip('\"\\'')\n",
    "            confidence = float(confidence_str)\n",
    "            confidence = max(0.0, min(1.0, confidence))\n",
    "            return label, confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response '{response_text}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def calculate_final_confidence_and_label(responses, labels_list):\n",
    "    \"\"\"Calculate final label and confidence from 5 responses\"\"\"\n",
    "    parsed_responses = []\n",
    "\n",
    "    for resp in responses:\n",
    "        if resp is None:\n",
    "            continue\n",
    "        label, confidence = parse_response_with_confidence(resp)\n",
    "        if label and confidence is not None and label in labels_list:\n",
    "            parsed_responses.append((label, confidence))\n",
    "\n",
    "    if not parsed_responses:\n",
    "        return None, 0.0\n",
    "\n",
    "    labels = [item[0] for item in parsed_responses]\n",
    "    confidences = [item[1] for item in parsed_responses]\n",
    "\n",
    "    # Only accept if all responses have confidence >= 0.95\n",
    "    if all(conf >= 0.95 for conf in confidences):\n",
    "        counter = Counter(labels)\n",
    "        most_common_label, count = counter.most_common(1)[0]\n",
    "        \n",
    "        try:\n",
    "            final_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating confidence: {e}\")\n",
    "            final_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "\n",
    "        final_confidence = max(0.0, min(1.0, final_confidence))\n",
    "        return most_common_label, final_confidence\n",
    "    else:\n",
    "        # If any response has confidence < 0.95, reject the classification\n",
    "        return None, 0.0\n",
    "\n",
    "def classify_single_entity(client, entity, labels_list):\n",
    "    \"\"\"Process a single entity and return result.\"\"\"\n",
    "    responses = process_entity_with_pass5(client, entity, labels_list)\n",
    "    final_label, final_confidence = calculate_final_confidence_and_label(responses, labels_list)\n",
    "\n",
    "    result = entity.copy()\n",
    "    result['assigned_label'] = final_label\n",
    "    result['confidence_score'] = final_confidence\n",
    "    result['raw_responses'] = responses\n",
    "\n",
    "    return result\n",
    "\n",
    "def classify_entities(client, entities_file, labels_file, max_workers=4):\n",
    "    \"\"\"Main function to classify all entities with deduplication and confidence scores using concurrency.\"\"\"\n",
    "\n",
    "    # Load entities line by line\n",
    "    entities = []\n",
    "    with open(entities_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                entities.append(json.loads(line.strip()))\n",
    "\n",
    "    print(f\"Loaded {len(entities)} entities from {entities_file}\")\n",
    "\n",
    "    # Deduplicate entities\n",
    "    unique_entities = deduplicate_entities(entities)\n",
    "\n",
    "    # Load labels\n",
    "    labels = load_labels_from_jsonl(labels_file)\n",
    "    print(f\"Loaded {len(labels)} labels from {labels_file}\")\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrent processing with tqdm progress bar\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_entity = {\n",
    "            executor.submit(classify_single_entity, client, entity, labels): entity\n",
    "            for entity in unique_entities\n",
    "        }\n",
    "\n",
    "        # Process with progress bar\n",
    "        for future in tqdm(as_completed(future_to_entity), total=len(future_to_entity), desc=\"Classifying entities\"):\n",
    "            try:\n",
    "                result = future.result(timeout=500)  # Optional timeout\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5955b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16995 entities from /home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_combined.jsonl\n",
      "Removed 831 duplicate entities\n",
      "Kept 16164 unique entities\n",
      "Loaded 60 labels from /home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_entity_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying entities: 100%|██████████| 16164/16164 [13:11<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken: 791.88 seconds\n",
      "\n",
      "Filtered results: 15436 out of 16164 passed confidence threshold\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8124/v1\",\n",
    "    api_key=\"dummy\"\n",
    ")\n",
    "\n",
    "# Run classification with concurrency\n",
    "start_time = time.time()\n",
    "results = classify_entities(\n",
    "    client,\n",
    "    '/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_combined.jsonl',\n",
    "    '/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_entity_labels.jsonl',\n",
    "    max_workers=32  # Adjust based on performance\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTotal time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Filter results to only include those with confidence >= 0.95\n",
    "filtered_results = [result for result in results if result.get('confidence_score', 0) >= 0.95]\n",
    "\n",
    "print(f\"\\nFiltered results: {len(filtered_results)} out of {len(results)} passed confidence threshold\")\n",
    "\n",
    "# Save filtered results to file\n",
    "with open('/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_filtered.jsonl', 'w') as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "# Also save all results (including rejected ones) for reference\n",
    "with open('/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_all.jsonl', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbbac1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
