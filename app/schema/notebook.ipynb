{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d793919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "from collections import Counter\n",
    "from hashlib import md5\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert entity classification system. Your task is to classify entities into predefined categories.\n",
    "\n",
    "## INSTRUCTIONS:\n",
    "1. You will receive a list of predefined labels at the beginning\n",
    "2. For each entity provided, you must select exactly ONE label from the predefined list\n",
    "3. Classifications must be from the exact predefined labels list\n",
    "4. If an entity doesn't clearly fit any label, choose the most appropriate one\n",
    "\n",
    "## PREDEFINED LABELS:\n",
    "{LABELS}\n",
    "\n",
    "## RESPONSE FORMAT:\n",
    "For each entity, provide exactly ONE label from the predefined list followed by a confidence score in this exact format:\n",
    "\"LABEL_NAME|CONFIDENCE_SCORE\"\n",
    "\n",
    "Where CONFIDENCE_SCORE is a number between 0 and 1 (e.g., \"Cemetery|0.95\")\n",
    "\n",
    "## CRITERIA FOR SELECTION:\n",
    "- Choose only from the predefined labels above\n",
    "- Select the most relevant label based on entity description\n",
    "- If multiple labels seem relevant, choose the most specific/precise one\n",
    "- If unsure, make your best educated guess from the available options\n",
    "- The label must be an exact match to one of the predefined labels\n",
    "- Confidence score should reflect how certain you are in your choice (0.0 = no knowledge at all, 0.5 = uncertain, 1.0 = very certain)\n",
    "\n",
    "## IMPORTANT:\n",
    "- ONLY return ONE label that exists in the predefined list\n",
    "- Do not create new labels or variations\n",
    "- Do not return anything other than exactly one valid label followed by a pipe and confidence score\n",
    "- The label must be a complete string matching exactly one of the predefined labels\n",
    "- The confidence score must be a decimal between 0 and 1\n",
    "- Return format: \"LABEL_NAME|CONFIDENCE_SCORE\" - nothing else\n",
    "\n",
    "## ENTITY TO CLASSIFY:\n",
    "{ENTITY_DATA}\n",
    "\"\"\"\n",
    "\n",
    "def load_labels_from_jsonl(file_path):\n",
    "    \"\"\"Load labels from JSONL file\"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            labels.append(data.get('label', data.get('name', '')))\n",
    "    return labels\n",
    "\n",
    "def create_system_prompt(labels_list, entity_data):\n",
    "    \"\"\"Create system prompt with labels\"\"\"\n",
    "    labels_str = \"\\n\".join([f\"- {label}\" for label in labels_list])\n",
    "    return SYSTEM_PROMPT.format(\n",
    "        LABELS=labels_str,\n",
    "        ENTITY_DATA=entity_data\n",
    "    )\n",
    "\n",
    "def get_entity_hash(entity_data):\n",
    "    \"\"\"Create a hash of the entity data for deduplication\"\"\"\n",
    "    entity_string = entity_data.get('entity', '').lower().strip()\n",
    "    return md5(entity_string.encode()).hexdigest()\n",
    "\n",
    "def deduplicate_entities(entities):\n",
    "    \"\"\"Remove duplicate entities based on the 'entity' field\"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique_entities = []\n",
    "    duplicates = []\n",
    "\n",
    "    for entity in entities:\n",
    "        entity_hash = get_entity_hash(entity)\n",
    "        if entity_hash not in seen_hashes:\n",
    "            seen_hashes.add(entity_hash)\n",
    "            unique_entities.append(entity)\n",
    "        else:\n",
    "            duplicates.append(entity)\n",
    "\n",
    "    print(f\"Removed {len(duplicates)} duplicate entities\")\n",
    "    print(f\"Kept {len(unique_entities)} unique entities\")\n",
    "\n",
    "    return unique_entities\n",
    "\n",
    "def process_entity_with_pass5(client, entity_data, labels_list):\n",
    "    \"\"\"Generate 5 classifications for an entity\"\"\"\n",
    "    simplified_data = {\n",
    "        \"entity\": entity_data.get(\"entity\", \"\"),\n",
    "        \"description\": entity_data.get(\"description\", \"\")\n",
    "    }\n",
    "\n",
    "    system_prompt = create_system_prompt(labels_list, json.dumps(simplified_data))\n",
    "\n",
    "    responses = []\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"google/gemma-3-12b-it\",  # Replace with your actual model name\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": f\"Classify this entity: {json.dumps(simplified_data)}\"}\n",
    "                ],\n",
    "                temperature=0.7,\n",
    "                max_tokens=150\n",
    "            )\n",
    "            response_text = completion.choices[0].message.content.strip()\n",
    "            responses.append(response_text)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating response {i+1}: {e}\")\n",
    "            responses.append(None)\n",
    "\n",
    "    return responses\n",
    "\n",
    "def parse_response_with_confidence(response_text):\n",
    "    \"\"\"Parse response to extract label and confidence score\"\"\"\n",
    "    if not response_text or \"|\" not in response_text:\n",
    "        return None, None\n",
    "\n",
    "    try:\n",
    "        parts = response_text.split(\"|\", 1)\n",
    "        if len(parts) >= 2:\n",
    "            label = parts[0].strip().strip('\"\\'')\n",
    "            confidence_str = parts[1].strip().strip('\"\\'')\n",
    "            confidence = float(confidence_str)\n",
    "            confidence = max(0.0, min(1.0, confidence))\n",
    "            return label, confidence\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response '{response_text}': {e}\")\n",
    "        return None, None\n",
    "\n",
    "    return None, None\n",
    "\n",
    "def calculate_final_confidence_and_label(responses, labels_list):\n",
    "    \"\"\"Calculate final label and confidence from 5 responses\"\"\"\n",
    "    parsed_responses = []\n",
    "\n",
    "    for resp in responses:\n",
    "        if resp is None:\n",
    "            continue\n",
    "        label, confidence = parse_response_with_confidence(resp)\n",
    "        if label and confidence is not None and label in labels_list:\n",
    "            parsed_responses.append((label, confidence))\n",
    "\n",
    "    if not parsed_responses:\n",
    "        return None, 0.0\n",
    "\n",
    "    labels = [item[0] for item in parsed_responses]\n",
    "    confidences = [item[1] for item in parsed_responses]\n",
    "\n",
    "    # Only accept if all responses have confidence >= 0.95\n",
    "    if all(conf >= 0.95 for conf in confidences):\n",
    "        counter = Counter(labels)\n",
    "        most_common_label, count = counter.most_common(1)[0]\n",
    "        \n",
    "        try:\n",
    "            final_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating confidence: {e}\")\n",
    "            final_confidence = sum(confidences) / len(confidences) if confidences else 0.0\n",
    "\n",
    "        final_confidence = max(0.0, min(1.0, final_confidence))\n",
    "        return most_common_label, final_confidence\n",
    "    else:\n",
    "        # If any response has confidence < 0.95, reject the classification\n",
    "        return None, 0.0\n",
    "\n",
    "def classify_single_entity(client, entity, labels_list):\n",
    "    \"\"\"Process a single entity and return result.\"\"\"\n",
    "    responses = process_entity_with_pass5(client, entity, labels_list)\n",
    "    final_label, final_confidence = calculate_final_confidence_and_label(responses, labels_list)\n",
    "\n",
    "    result = entity.copy()\n",
    "    result['assigned_label'] = final_label\n",
    "    result['confidence_score'] = final_confidence\n",
    "    result['raw_responses'] = responses\n",
    "\n",
    "    return result\n",
    "\n",
    "def classify_entities(client, entities_file, labels_file, max_workers=4):\n",
    "    \"\"\"Main function to classify all entities with deduplication and confidence scores using concurrency.\"\"\"\n",
    "\n",
    "    # Load entities line by line\n",
    "    entities = []\n",
    "    with open(entities_file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                entities.append(json.loads(line.strip()))\n",
    "\n",
    "    print(f\"Loaded {len(entities)} entities from {entities_file}\")\n",
    "\n",
    "    # Deduplicate entities\n",
    "    unique_entities = deduplicate_entities(entities)\n",
    "\n",
    "    # Load labels\n",
    "    labels = load_labels_from_jsonl(labels_file)\n",
    "    print(f\"Loaded {len(labels)} labels from {labels_file}\")\n",
    "\n",
    "    # Use ThreadPoolExecutor for concurrent processing with tqdm progress bar\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_entity = {\n",
    "            executor.submit(classify_single_entity, client, entity, labels): entity\n",
    "            for entity in unique_entities\n",
    "        }\n",
    "\n",
    "        # Process with progress bar\n",
    "        for future in tqdm(as_completed(future_to_entity), total=len(future_to_entity), desc=\"Classifying entities\"):\n",
    "            try:\n",
    "                result = future.result(timeout=500)  # Optional timeout\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f\"Generated an exception: {exc}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5955b191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 16995 entities from /home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_combined.jsonl\n",
      "Removed 831 duplicate entities\n",
      "Kept 16164 unique entities\n",
      "Loaded 60 labels from /home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_entity_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifying entities: 100%|██████████| 16164/16164 [13:11<00:00, 20.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total time taken: 791.88 seconds\n",
      "\n",
      "Filtered results: 15436 out of 16164 passed confidence threshold\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize client\n",
    "client = OpenAI(\n",
    "    base_url=\"http://localhost:8124/v1\",\n",
    "    api_key=\"dummy\"\n",
    ")\n",
    "\n",
    "# Run classification with concurrency\n",
    "start_time = time.time()\n",
    "results = classify_entities(\n",
    "    client,\n",
    "    '/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_combined.jsonl',\n",
    "    '/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/_entity_labels.jsonl',\n",
    "    max_workers=32  # Adjust based on performance\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"\\nTotal time taken: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Filter results to only include those with confidence >= 0.95\n",
    "filtered_results = [result for result in results if result.get('confidence_score', 0) >= 0.95]\n",
    "\n",
    "print(f\"\\nFiltered results: {len(filtered_results)} out of {len(results)} passed confidence threshold\")\n",
    "\n",
    "# Save filtered results to file\n",
    "with open('/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_filtered.jsonl', 'w') as f:\n",
    "    for result in filtered_results:\n",
    "        f.write(json.dumps(result) + '\\n')\n",
    "\n",
    "# Also save all results (including rejected ones) for reference\n",
    "with open('/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_all.jsonl', 'w') as f:\n",
    "    for result in results:\n",
    "        f.write(json.dumps(result) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40dbbac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your JSONL file\n",
    "path = \"/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_filtered.jsonl\"\n",
    "\n",
    "rows = []\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        # Only keep the keys you care about\n",
    "        rows.append({\n",
    "            \"entity\": obj.get(\"entity\", \"\"),\n",
    "            \"label\": obj.get(\"label\", \"\")\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe633a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aamer Architects Pte Ltd</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NSY Architects</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a+pgrp</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3DNA</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Studio Limau</td>\n",
       "      <td>Company</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     entity    label\n",
       "0  Aamer Architects Pte Ltd  Company\n",
       "1            NSY Architects  Company\n",
       "2                    a+pgrp  Company\n",
       "3                      3DNA  Company\n",
       "4              Studio Limau  Company"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(rows)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6fb4473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "HDB Block                    6542\n",
       "Preschool                    1752\n",
       "Nonprofit Organization       1636\n",
       "Company                      1112\n",
       "Hotel                         516\n",
       "Private Institute             393\n",
       "Park                          388\n",
       "Office Tower                  348\n",
       "Healthcare Facility           276\n",
       "Condominium                   199\n",
       "Retail Mall                   179\n",
       "Local Street                  160\n",
       "Public Facilities             142\n",
       "MRT Station                   140\n",
       "National Monument             139\n",
       "Primary School                128\n",
       "University                    122\n",
       "Government Service Centre     121\n",
       "Hawker Centre                 120\n",
       "Financial Institution         119\n",
       "Secondary School              104\n",
       "Tourist Attraction             99\n",
       "Museum or Heritage Centre      79\n",
       "Community Club                 77\n",
       "Religious Site                 76\n",
       "Supermarket                    75\n",
       "Government Building            75\n",
       "Bus Stop                       56\n",
       "Address or Location            48\n",
       "LRT Station                    41\n",
       "Political Party                40\n",
       "Government Agency              37\n",
       "Airport                        16\n",
       "Drug Substance                 12\n",
       "Public Holiday                 12\n",
       "Maritime Operator               8\n",
       "Landed Property                 7\n",
       "Ministry                        7\n",
       "Local Cuisine                   7\n",
       "Sports                          6\n",
       "Reservoir                       5\n",
       "Junior College                  5\n",
       "Expressway                      4\n",
       "Ferry Terminal                  4\n",
       "Media                           2\n",
       "Social Support Policy           2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ba9171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>604</th>\n",
       "      <td>Singapore Sustainability Academy</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>BCA Academy, Academic Tower</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>652</th>\n",
       "      <td>National Community Leadership Institute</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>Singapore Institute of Technology @ NP Building</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>Institute of Microelectronics (IME)</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14608</th>\n",
       "      <td>Thong Kheng Student Care Centres</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14684</th>\n",
       "      <td>Student Care Centre @ Tampines Street 82</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14738</th>\n",
       "      <td>Tanglin School</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15136</th>\n",
       "      <td>Madrasah Aljunied al-Islamiah</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15312</th>\n",
       "      <td>Centre for Contemporary Art</td>\n",
       "      <td>Private Institute</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>393 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                entity              label\n",
       "604                   Singapore Sustainability Academy  Private Institute\n",
       "645                        BCA Academy, Academic Tower  Private Institute\n",
       "652            National Community Leadership Institute  Private Institute\n",
       "720    Singapore Institute of Technology @ NP Building  Private Institute\n",
       "730                Institute of Microelectronics (IME)  Private Institute\n",
       "...                                                ...                ...\n",
       "14608                 Thong Kheng Student Care Centres  Private Institute\n",
       "14684         Student Care Centre @ Tampines Street 82  Private Institute\n",
       "14738                                   Tanglin School  Private Institute\n",
       "15136                    Madrasah Aljunied al-Islamiah  Private Institute\n",
       "15312                      Centre for Contemporary Art  Private Institute\n",
       "\n",
       "[393 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"label\"] == \"Private Institute\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0731a334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 6378 items to /home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_filtered_sampled_500.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# Path to your JSONL file\n",
    "path = \"/home/leeeefun681/volume/eefun/webscraping/scraping/vlm_webscrape/app/schema/classified_entities_filtered.jsonl\"\n",
    "\n",
    "# Read all data and group by label\n",
    "label_groups = defaultdict(list)\n",
    "\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        obj = json.loads(line)\n",
    "        # Only keep the keys you care about\n",
    "        label_groups[obj.get(\"label\", \"\")].append({\n",
    "            \"entity\": obj.get(\"entity\", \"\"),\n",
    "            \"label\": obj.get(\"label\", \"\")\n",
    "        })\n",
    "\n",
    "# Sample at most 500 from each label group\n",
    "sampled_data = []\n",
    "for label, entries in label_groups.items():\n",
    "    # Randomly sample up to 500 items (or all if less than 500)\n",
    "    sampled_entries = random.sample(entries, min(500, len(entries)))\n",
    "    sampled_data.extend(sampled_entries)\n",
    "\n",
    "# Shuffle the final dataset\n",
    "random.shuffle(sampled_data)\n",
    "\n",
    "# Save to new JSONL file in the same directory\n",
    "output_path = path.replace(\".jsonl\", \"_sampled_500.jsonl\")\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in sampled_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(sampled_data)} items to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
